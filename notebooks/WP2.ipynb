{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f85697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.inforet import download_and_extract, load_split, batched_topk_indices, precision_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a236dba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Stanford IMDb dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\charl\\\\Desktop\\\\A4 - DIA\\\\DIA\\\\Information-retrieval-NLP\\\\Data\\\\aclImdb_v1.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m X_train, y_train = load_split(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m X_test, y_test = load_split(\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\charl\\Desktop\\A4 - DIA\\DIA\\Information-retrieval-NLP\\src\\inforet.py:18\u001b[39m, in \u001b[36mdownload_and_extract\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(ARCHIVE):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDownloading Stanford IMDb dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mURL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mARCHIVE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(DATA_DIR):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExtracting dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py:224\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Handle temporary file setup.\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     tfp = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    226\u001b[39m     tfp = tempfile.NamedTemporaryFile(delete=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\charl\\\\Desktop\\\\A4 - DIA\\\\DIA\\\\Information-retrieval-NLP\\\\Data\\\\aclImdb_v1.tar.gz'"
     ]
    }
   ],
   "source": [
    "download_and_extract()\n",
    "\n",
    "X_train, y_train = load_split(\"train\")\n",
    "X_test, y_test = load_split(\"test\")\n",
    "\n",
    "print(f\"Train: {len(X_train)} docs | Test (queries): {len(X_test)} docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF index on TRAIN\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=50)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5448ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11568  2879  5527 ... 23234 23726 10764]\n",
      " [15573 18477  4416 ...  2434 13186 19149]\n",
      " [10211  3710 14641 ...   413 21732 10747]\n",
      " ...\n",
      " [11661 12649 10275 ... 14729 15870 18551]\n",
      " [ 9203 10155 22374 ... 10650  7909  1264]\n",
      " [21417  2683 22143 ...  4680 22838 19984]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate retrieval-by-sentiment\n",
    "K = 10\n",
    "topk = batched_topk_indices(X_train_tfidf, X_test_tfidf, k = K)\n",
    "\n",
    "print(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_p_at_k = precision_at_k(topk, y_train, y_test)\n",
    "\n",
    "pos_mask = (y_test == 1)\n",
    "neg_mask = (y_test == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_k_pos = precision_at_k(topk[pos_mask], y_train, y_test[pos_mask])\n",
    "p_at_k_neg = precision_at_k(topk[neg_mask], y_train, y_test[neg_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748af665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision@10 (label match):\n",
      " Overall: 0.5844\n",
      " Pos queries -> Pos retrieved: 0.5806\n",
      " Neg queries -> Neg retrieved: 0.5881\n",
      "\n",
      "Example query:\n",
      " Query label:  pos\n",
      " Query text:  I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. ...\n",
      " Top retrieved labels:  ['pos', 'pos', 'pos', 'pos', 'neg', 'pos', 'pos', 'neg', 'neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPrecision@{K} (label match):\")\n",
    "print(f\" Overall: {overall_p_at_k:.4f}\")\n",
    "print(f\" Pos queries -> Pos retrieved: {p_at_k_pos:.4f}\")\n",
    "print(f\" Neg queries -> Neg retrieved: {p_at_k_neg:.4f}\")\n",
    "\n",
    "# Optional : show one example query + retrieved labels\n",
    "\n",
    "qi = 0\n",
    "retrieved = topk[qi]\n",
    "print(\"\\nExample query:\")\n",
    "print(\" Query label: \", \"pos\" if y_test[qi] == 1 else \"neg\")\n",
    "print(\" Query text: \", X_test[qi][:200].replace(\"\\n\", \" \"), \"...\")\n",
    "print(\" Top retrieved labels: \", [\"pos\" if y_train[i] == 1 else \"neg\" for i in retrieved])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4afa6",
   "metadata": {},
   "source": [
    "## Exercise 1:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
